{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce27ad-2ef9-42c1-82a9-15ced3f15f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.databricks#dbutils-api_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0a3b496e-d15f-4766-a484-0ceb71523cde;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#dbutils-api_2.12;0.0.5 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/dbutils-api_2.12/0.0.5/dbutils-api_2.12-0.0.5.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#dbutils-api_2.12;0.0.5!dbutils-api_2.12.jar (117ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (912ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (180ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (55885ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (249ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (423ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (179ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (123ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (82ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (106ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (3722ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (113ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (981ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (139ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (1276ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (259ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (809ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (104ms)\n",
      ":: resolution report :: resolve 22305ms :: artifacts dl 65912ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.databricks#dbutils-api_2.12;0.0.5 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   20  |   20  |   1   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0a3b496e-d15f-4766-a484-0ceb71523cde\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (198147kB/245ms)\n",
      "21/10/25 07:38:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Column, DataFrame, SparkSession, functions\n",
    "from pyspark.sql.functions import *\n",
    "from py4j.java_collections import MapConverter\n",
    "import shutil\n",
    "import random\n",
    "import threading\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://spark:7077\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", 'http://s3:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'minio') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'minio123') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bf6254-be9f-4876-a02c-70f6e727db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc).builder.appName(\"streaming\").getOrCreate()\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c71d4b-5681-468b-ace9-92fe325a9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############  Original Delta Table ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+--------------------+\n",
      "|  id|first_name|last_name|               email|\n",
      "+----+----------+---------+--------------------+\n",
      "|1001|     Sally|   Thomas|sally.thomas@acme...|\n",
      "|1004|      Anne|Kretchmar|  annek@noanswer.org|\n",
      "|1002|    George|   Bailey|  gbailey@foobar.com|\n",
      "|1003|    Edward|   Walker|       ed@walker.com|\n",
      "+----+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#raw data received from kafka is stored under S3 customers\n",
    "OBJECTURL_TEST = 's3a://minio-sink-bucket/topics/customers'\n",
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "streamingRawDF=spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)\n",
    "stream=streamingRawDF.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\")).writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers/checkpoints/').start('s3a://minio-sink-bucket/delta/bronze/customers/data/')\n",
    "#set it to low number in order to see the acidity\n",
    "#for 5 seconds it will transfer some of the data and with fail, and the delta will not be created\n",
    "stream.awaitTermination(20)\n",
    "stream.stop()\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://minio-sink-bucket/delta/bronze/customers/data')\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ab6ff-76c0-4646-b9e4-059b7bd5b380",
   "metadata": {},
   "source": [
    "the following is an example of stream data with upsert, in order to avoid duplication by id\n",
    "the input is CSV file and the output is bronze delta with only rellevant id's\n",
    "we might want to change the CSV to Avro, as its more efficient, but less readable\n",
    "now, execute the insert and update from the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a505da-7eed-49d0-bf99-5e649a58a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Streaming upgrades in update mode ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/25 08:04:38 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-61d79c58-ce94-40d3-81b7-4f0963f6239f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----------+---------+--------------------+\n",
      "|  id|first_name|last_name|               email|\n",
      "+----+----------+---------+--------------------+\n",
      "|1005|      Jane|      Roe|john.doe@example.com|\n",
      "|1001|     Sally|   Thomas|sally.thomas@acme...|\n",
      "|1004|      Anne|Kretchmar|  annek@noanswer.org|\n",
      "|1002|    George|   Bailey|  gbailey@foobar.com|\n",
      "|1003|    Edward|   Walker|       ed@walker.com|\n",
      "+----+----------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/25 08:04:50 ERROR MicroBatchExecution: Query [id = d8fa7911-048e-4eaf-a5c0-2a4edc369d32, runId = eac2a6f0-12a6-4624-8352-bccf104a60e0] terminated with error\n",
      "java.lang.UnsupportedOperationException: Detected a data update (for example part-00000-cf1340f7-5a58-4d11-8e8a-8c93bf810d9d-c000.snappy.parquet) in the source table at version 3. This is currently not supported. If you'd like to ignore updates, set the option 'ignoreChanges' to 'true'. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.deltaSourceIgnoreChangesError(DeltaErrors.scala:137)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.verifyStreamHygieneAndFilterAddFiles(DeltaSource.scala:348)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.$anonfun$getFileChanges$1(DeltaSource.scala:191)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:198)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:225)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$15.hasNext(Iterator.scala:653)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.iteratorLast(DeltaSource.scala:229)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.latestOffset(DeltaSource.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:385)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:382)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:613)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:378)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "21/10/25 08:04:50 ERROR MicroBatchExecution: Query [id = 2751ad7d-a1f3-40c0-b43a-a9f1b7c34ec1, runId = 11067331-d66e-45e5-a5bf-14406b68894d] terminated with error\n",
      "java.lang.UnsupportedOperationException: Detected a data update (for example part-00000-cf1340f7-5a58-4d11-8e8a-8c93bf810d9d-c000.snappy.parquet) in the source table at version 3. This is currently not supported. If you'd like to ignore updates, set the option 'ignoreChanges' to 'true'. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.deltaSourceIgnoreChangesError(DeltaErrors.scala:137)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.verifyStreamHygieneAndFilterAddFiles(DeltaSource.scala:348)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.$anonfun$getFileChanges$1(DeltaSource.scala:191)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:198)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:225)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$15.hasNext(Iterator.scala:653)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.iteratorLast(DeltaSource.scala:229)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.latestOffset(DeltaSource.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:385)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:382)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:613)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:378)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Detected a data update (for example part-00000-cf1340f7-5a58-4d11-8e8a-8c93bf810d9d-c000.snappy.parquet) in the source table at version 3. This is currently not supported. If you'd like to ignore updates, set the option 'ignoreChanges' to 'true'. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory.\n=== Streaming Query ===\nIdentifier: [id = d8fa7911-048e-4eaf-a5c0-2a4edc369d32, runId = eac2a6f0-12a6-4624-8352-bccf104a60e0]\nCurrent Committed Offsets: {DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data]: {\"sourceVersion\":1,\"reservoirId\":\"2c0b8639-0409-4cb5-ac04-4c57070182db\",\"reservoirVersion\":2,\"index\":4,\"isStartingVersion\":true}}\nCurrent Available Offsets: {DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data]: {\"sourceVersion\":1,\"reservoirId\":\"2c0b8639-0409-4cb5-ac04-4c57070182db\",\"reservoirVersion\":2,\"index\":4,\"isStartingVersion\":true}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- StreamingExecutionRelation DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data], [id#2873L, first_name#2874, last_name#2875, email#2876]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_67/440370231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mstream3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mstream3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mstream2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mstream2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"########### DeltaTable after streaming upsert #########\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeout must be a positive integer or float. Got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Detected a data update (for example part-00000-cf1340f7-5a58-4d11-8e8a-8c93bf810d9d-c000.snappy.parquet) in the source table at version 3. This is currently not supported. If you'd like to ignore updates, set the option 'ignoreChanges' to 'true'. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory.\n=== Streaming Query ===\nIdentifier: [id = d8fa7911-048e-4eaf-a5c0-2a4edc369d32, runId = eac2a6f0-12a6-4624-8352-bccf104a60e0]\nCurrent Committed Offsets: {DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data]: {\"sourceVersion\":1,\"reservoirId\":\"2c0b8639-0409-4cb5-ac04-4c57070182db\",\"reservoirVersion\":2,\"index\":4,\"isStartingVersion\":true}}\nCurrent Available Offsets: {DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data]: {\"sourceVersion\":1,\"reservoirId\":\"2c0b8639-0409-4cb5-ac04-4c57070182db\",\"reservoirVersion\":2,\"index\":4,\"isStartingVersion\":true}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- StreamingExecutionRelation DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data], [id#2873L, first_name#2874, last_name#2875, email#2876]\n"
     ]
    }
   ],
   "source": [
    "# Streaming aggregates in Update mode\n",
    "print(\"####### Streaming upgrades in update mode ########\")\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "# if id doesnt exist it inserts if it does, update\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "#schema is taken from files that already exist on the topic \n",
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "#reading raw data (json format)\n",
    "streamingRawDF=spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)\n",
    "#taking the rellevant data from raw data\n",
    "streamingAggregatesDF=streamingRawDF.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\"))\n",
    "# Write the output of a streaming aggregation query into Delta Lake bronze table\n",
    "#call upsert for each and every new record\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers/checkpoints/') \\\n",
    "    .start('s3a://minio-sink-bucket/delta/bronze/customers/data/')\n",
    "#wait for 10 seconds before continue to stop\n",
    "stream2 = spark.readStream.format(\"delta\").load('s3a://minio-sink-bucket/delta/bronze/customers/data/')\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "\n",
    "stream3.awaitTermination(30)\n",
    "stream3.stop()\n",
    "stream2.awaitTermination(300)\n",
    "stream2.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "#deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e366a-4ff6-40c6-b2b0-938916311cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53160895-6f94-4111-a3c9-376aea8f3549",
   "metadata": {},
   "source": [
    "example for updating according to input event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8e6ae-16fc-481d-80b7-7b5ddb09efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming aggregates in Update mode\n",
    "print(\"####### Streaming upgrades in update mode ########\")\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "# if id doesnt exist it inserts if it does, update\n",
    "def upsertToDeltaByBefore(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), 's.id = t.id')\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "#raw data received from kafka is stored under S3 customers\n",
    "OBJECTURL_TEST = 's3a://minio-sink-bucket/topics/customers'\n",
    "#schema is taken from files that already exist on the topic \n",
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "#reading raw data (json format)\n",
    "streamingRawDF=spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)\n",
    "#taking the rellevant data from raw data\n",
    "streamingAggregatesDF=streamingRawDF.select(col('before'), col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\"))\n",
    "# Write the output of a streaming aggregation query into Delta Lake bronze table\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://minio-sink-bucket/delta/bronze/customers/data/')\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n",
    "#call upsert for each and every new record\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDeltaByBefore) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "#wait for 100 seconds before continue to stop\n",
    "stream3.awaitTermination(100)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04b0f6-d4c7-4ad2-8246-074aa66b878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"json\").schema(schema).load(OBJECTURL_TEST).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56780423-6ddf-4624-8ed3-d13fe0a6a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa851701-0949-4d3e-94c3-7a2dfe063ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdd2ed-3d64-4d36-b2e2-cbc31fbb2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.vacuum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ee41a-7a03-4df8-bf96-0990d3cf7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(numRows).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac1027-f5be-4379-93b6-ce5ade703580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming append and concurrent repartition using  data change = false\n",
    "# tbl1 is the sink and tbl2 is the source\n",
    "print(\"############ Streaming appends with concurrent table repartition  ##########\")\n",
    "tbl1 = 's3a://minio-sink-bucket/test/delta-table4'\n",
    "tbl2 = \"s3a://minio-sink-bucket/test/delta-table5\"\n",
    "numRows = 10\n",
    "spark.range(numRows).write.mode(\"overwrite\").format(\"delta\").save(tbl1)\n",
    "spark.read.format(\"delta\").load(tbl1).show()\n",
    "spark.range(numRows, numRows * 10).write.mode(\"overwrite\").format(\"delta\").save(tbl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcdf9cc-9c58-44e8-a756-97cbfb2097c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start reading tbl2 as a stream and do a streaming write to tbl1\n",
    "# Prior to Delta 0.5.0 this would throw StreamingQueryException: Detected a data update in the\n",
    "# source table. This is currently not supported.\n",
    "stream4 = spark.readStream.format(\"delta\").load(tbl2).writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://minio-sink-bucket/test/checkpoint/tbl1\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(tbl1)\n",
    "\n",
    "# repartition table while streaming job is running\n",
    "spark.read.format(\"delta\").load(tbl2).repartition(10).write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"dataChange\", \"false\")\\\n",
    "    .save(tbl2)\n",
    "\n",
    "stream4.awaitTermination(10)\n",
    "stream4.stop()\n",
    "print(\"######### After streaming write #########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d0e09-08d1-430d-92fe-a6e04354c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(tbl1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd37b3d-4e15-4e5d-aabf-31c66c57fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://minio-sink-bucket/topics/customers'\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('json') \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043bfad-9c90-40ca-81e9-65946453ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "streamingDf=spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)\n",
    "stream=sdf.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\")).writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers/checkpoints/').start('s3a://minio-sink-bucket/delta/bronze/customers/data/')\n",
    "stream.awaitTermination(10)\n",
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a7b30-755a-4aae-956a-79a650e107f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db37fa-007e-458c-8966-d5997d9f53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream reads from a table\n",
    "print(\"##### Reading from stream ######\")\n",
    "stream2 = spark.readStream.format(\"delta\").load('s3a://minio-sink-bucket/delta/bronze/customers/data/')\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "stream2.awaitTermination()\n",
    "#stream2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec548c0-641e-452a-91a6-59dff45137d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"s3a://minio-sink-bucket/delta/bronze/customers/data/\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1b6ad-ef1e-49d4-a538-f4254e33dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingAggregatesDF = spark.readStream.format(\"s3a://minio-sink-bucket/delta/bronze/customers/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896eb33-8091-45eb-b506-14786fe9bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c20055-e7ac-45e2-a224-1dc99e26bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming aggregates in Update mode\n",
    "print(\"####### Streaming upgrades in update mode ########\")\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331c547-efcc-4940-9b31-3de7e0f8046d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0048a3d-48aa-4ff1-8e50-e9ec39ca4c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbfc76-26ea-4857-9c0c-4b814170ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f875bd3-1447-46c5-839f-2f873af7fa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99331da-f246-4566-af6a-966b77b3fddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743929f5-e7fd-4209-b5f1-71ff73beaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c53eb8-ef96-4603-8930-e58ba77c3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load('s3a://minio-sink-bucket/delta/bronze/customers/data/').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b15b2-db61-4699-a9bf-c01696a44f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = spark.readStream.format(\"cloudFiles\")\\\n",
    "   .option('cloudFiles.format', 'text')\\\n",
    "   .load('/mnt/tlogs/staging')\n",
    " # updated to remove EntryMethod which apear twice and not allowed by delta lake\n",
    " \n",
    " bronze=df.select(from_json(regexp_replace('value', 'EntryMethod', 'rabEntryM'),schema_s).alias('value')).\\\n",
    "           select(col('value._id').alias(\"tlogId\"),col('value.organization').alias(\"siteId\"),to_date('value.businessDate').\\\n",
    "           alias(\"businessDate\"),explode('value.order').alias('order'))\\\n",
    "           .withColumn('orderType', col('order.orderType')).withColumn('orderTotalAmount', col('order.totals.totalAmount'))\\\n",
    "           .withColumn('orderId',col('order._id')).writeStream.partitionBy('siteId').format(\"delta\").\\\n",
    "            option(\"checkpointLocation\", '/mnt/tlogs/checkpoints/bronze_cp').option('multiline', True).start('/mnt/tlogs/deltas/bronze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706eb5e-84d2-4424-a1a6-707b51f2f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "gnames_df.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbfd2a-0e29-493a-bb1f-ce4d805b6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62934cf2-878a-4fe7-9b76-011c5b220f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import koalas as ks\n",
    "kdf = gnames_df.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fcf5f-52d5-4309-8f41-41b21e4a3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf['after'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec704d5-09f6-40eb-9def-94660f083489",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dir = \"s3a://test-container/playground/delta-table\"\n",
    "spark.sql(\"CREATE TABLE delta.`%s`(id LONG) USING delta\" % table_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202326-5d5e-4c7c-bdf3-ef1176583510",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"VACUUM '%s' RETAIN 169 HOURS\" % table_dir).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2571b-acc6-4ab5-be0e-aa86721edeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", data.id + random.randint(0, 5000))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1971d-f5db-4097-88a7-eb8fd5d3177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO delta.`%s` VALUES 0, 1, 2, 3, 4\" % table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ce1a4-1794-451d-bdee-b1c1f30701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM delta.`%s`\" % table_dir).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e81ba-ae6f-4226-b3e5-cc4d050f7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c144e-5f0c-4e84-a702-12a9f29faef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://test-container/playground/colors-test' + str(time.time()) + '.csv'\n",
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.csv(OBJECTURL_TEST, header=True)\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('csv').option('header', True) \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc16fcc-4e9e-43c5-85b7-4c1f90d7c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_URL='s3a://test-container/playground/delts-colors-test' + str(time.time())\n",
    "gnames_df.write.format(\"delta\").save(DELTA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac4e11-027a-4a9f-820a-16fb74d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "#data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afaad5-9379-499f-9e1d-9bef4dca173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"delta\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ec175-bf98-4bfd-b0ae-5aa8777a5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb7f69-501d-4c60-bdc0-f4747827bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import koalas as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea596c-5197-4227-89ba-f4699106a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ks.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43e847-b267-4e25-a3e1-2b0b69b7d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb304b-41fc-4b5d-be18-fc728e1e67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ddd68-8aaf-44ef-a853-e4ef9318e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "kdf = ks.from_pandas(pdf)\n",
    "print(type(kdf))\n",
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfc358-de4a-4ed8-93d1-6142d2ecbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bf484-8341-4c01-b89f-390c7eb0829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = sdf.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0a05-a9b5-4352-a424-3ad2c0c08be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c9e68-c87c-4830-9335-b80ba2987513",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_parquet(\"s3a://test-container/playground/kdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f4818-f300-49b1-afaf-c3de37fa03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.read_parquet(\"s3a://test-container/playground/kdf\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f040a36-7aaf-4861-9992-1f0621546382",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_delta(\"s3a://test-container/playground/delta_partitioned\", mode='overwrite', partition_cols=[\"A\",\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1563acc-f892-44e7-abc0-4f11c9517bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.read_delta(\"s3a://test-container/playground/delta\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed627b8-51e2-48d3-a3ad-76282b3baff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39da425-79a6-4b7b-9ed9-cdb78f3b9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", data.id + random.randint(0, 5000))\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/delta-streaming/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc78bdc-d705-4a23-8143-99987e910770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream writes to the table\n",
    "print(\"####### Streaming write ######\")\n",
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-container/playground/delta-streaming/checkpoint\")\\\n",
    "    .start(\"s3a://test-container/playground/delta-streaming/delta-table2\")\n",
    "stream.awaitTermination(10)\n",
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4862c78-d556-434a-b926-797bce8ab028",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM delta.`%s`\" % \"s3a://test-container/playground/delta-streaming/delta-table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2d73b-8074-4ec2-bdc2-6059c03960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream reads from a table\n",
    "print(\"##### Reading from stream ######\")\n",
    "stream2 = spark.readStream.format(\"delta\").load(\"s3a://test-container/playground/delta-streaming/delta-table2\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "stream2.awaitTermination(10)\n",
    "stream2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71689bbc-453e-44f4-9ad9-e4a8e0fb82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####### Streaming upgrades in update mode ########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712c105-2dcf-42a9-ae04-6daa80db7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e06e-d46d-47d4-8f66-2f032fb67d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "streamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n",
    "    .withColumn(\"id\", col(\"value\") % 10)\\\n",
    "    .drop(\"timestamp\")\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://test-container/playground/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176b3e9-51fb-4bcd-a70b-dd4702a7dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155970e-ca3b-41a9-83f9-8cc7979fc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n",
    "    .withColumn(\"id\", col(\"value\") % 10)\\\n",
    "    .drop(\"timestamp\")\n",
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://test-container/playground/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n",
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")\n",
    "deltaTable.toDF().show()\n",
    "\n",
    "# Streaming append and concurrent repartition using  data change = false\n",
    "# tbl1 is the sink and tbl2 is the source\n",
    "print(\"############ Streaming appends with concurrent table repartition  ##########\")\n",
    "tbl1 = \"s3a://test-container/playground/delta-streaming/delta-table4\"\n",
    "tbl2 = \"s3a://test-container/playground/delta-streaming/delta-table5\"\n",
    "numRows = 10\n",
    "spark.range(numRows).write.mode(\"overwrite\").format(\"delta\").save(tbl1)\n",
    "spark.read.format(\"delta\").load(tbl1).show()\n",
    "spark.range(numRows, numRows * 10).write.mode(\"overwrite\").format(\"delta\").save(tbl2)\n",
    "\n",
    "\n",
    "# Start reading tbl2 as a stream and do a streaming write to tbl1\n",
    "# Prior to Delta 0.5.0 this would throw StreamingQueryException: Detected a data update in the\n",
    "# source table. This is currently not supported.\n",
    "stream4 = spark.readStream.format(\"delta\").load(tbl2).writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-container/playground/delta-streaming/checkpoint/tbl1\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(tbl1)\n",
    "\n",
    "# repartition table while streaming job is running\n",
    "spark.read.format(\"delta\").load(tbl2).repartition(10).write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"dataChange\", \"false\")\\\n",
    "    .save(tbl2)\n",
    "\n",
    "stream4.awaitTermination(10)\n",
    "stream4.stop()\n",
    "print(\"######### After streaming write #########\")\n",
    "spark.read.format(\"delta\").load(tbl1).show()\n",
    "# cleanup\n",
    "try:\n",
    "    shutil.rmtree(\"s3a://test-container/playground/delta-streaming/\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115d7f5-c138-4cea-9279-8100f3aabd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE delta.`%s`(id LONG)\" % \"s3a://test-container/playground/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4cfa7c-8598-411a-8f3c-b70b0111a50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

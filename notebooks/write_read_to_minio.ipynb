{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce27ad-2ef9-42c1-82a9-15ced3f15f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.databricks#dbutils-api_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7ba30dcd-1be7-4714-ab94-977b748b8ee0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#dbutils-api_2.12;0.0.5 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/dbutils-api_2.12/0.0.5/dbutils-api_2.12-0.0.5.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#dbutils-api_2.12;0.0.5!dbutils-api_2.12.jar (113ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (996ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (199ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (68196ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (247ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (566ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (221ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (151ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (107ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (4449ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (101ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (1419ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (120ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (1720ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (327ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (863ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (91ms)\n",
      ":: resolution report :: resolve 22963ms :: artifacts dl 80300ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.databricks#dbutils-api_2.12;0.0.5 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   20  |   20  |   1   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7ba30dcd-1be7-4714-ab94-977b748b8ee0\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (198147kB/397ms)\n",
      "21/10/31 06:31:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Column, DataFrame, SparkSession, functions\n",
    "from pyspark.sql.functions import *\n",
    "from py4j.java_collections import MapConverter\n",
    "import shutil\n",
    "import random\n",
    "import threading\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://spark:7077\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", 'http://s3:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'minio') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'minio123') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bf6254-be9f-4876-a02c-70f6e727db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc).builder.appName(\"streaming\").getOrCreate()\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c71d4b-5681-468b-ace9-92fe325a9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############  Original Delta Table ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+--------------------+--------------------+\n",
      "|  id|first_name|last_name|               email|            inserted|\n",
      "+----+----------+---------+--------------------+--------------------+\n",
      "|1001|      Jane|  changed|sally.thomas@acme...|2021-10-31 06:35:...|\n",
      "|1001|     Sally|   Thomas|sally.thomas@acme...|2021-10-31 06:31:...|\n",
      "|1005|      John|      Doe|john.doe@example.com|2021-10-31 06:35:...|\n",
      "|1002|    George|   Bailey|  gbailey@foobar.com|2021-10-31 06:31:...|\n",
      "|1004|      Anne|Kretchmar|  annek@noanswer.org|2021-10-31 06:31:...|\n",
      "|1003|    Edward|   Walker|       ed@walker.com|2021-10-31 06:31:...|\n",
      "+----+----------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#raw data received from kafka is stored under S3 customers\n",
    "OBJECTURL_TEST = 's3a://minio-sink-bucket/topics/customers'\n",
    "schema=spark.read.format('json').load(OBJECTURL_TEST).schema\n",
    "streamingRawDF=spark.readStream.format(\"json\").schema(schema).load(OBJECTURL_TEST)\n",
    "stream=streamingRawDF.select(col(\"after.id\"), col(\"after.first_name\"), col(\"after.last_name\"), col(\"after.email\")).withColumn('inserted', current_timestamp()).writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .partitionBy(['last_name', 'first_name']) \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .options(ignoreDeletes=True) \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers/checkpoints/').start('s3a://minio-sink-bucket/delta/bronze/customers/data/')\n",
    "stream.awaitTermination()\n",
    "#stream.stop()\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://minio-sink-bucket/delta/bronze/customers/data')\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee17137a-76c8-4543-9994-ef3382291ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.readStream.format(\"delta\").load('s3a://minio-sink-bucket/delta/bronze/customers/data')\n",
    "#df=df.withColumn('inserted', current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b47327-c09a-476e-9993-c8fb178291d9",
   "metadata": {},
   "source": [
    "call this only on the 1st time in order to create the table.\n",
    "need to find away to identify if the path is empty, similar to databricks dbuitls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbba3635-9a8b-4b93-9ec4-51d0a0cc1839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fcad8361790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers_stream/checkpoints/') \\\n",
    "  .start('s3a://minio-sink-bucket/delta/bronze/customers_stream/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0c4b11-630f-4178-9639-b9681fc801e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = 'customerstream'\n",
    "save_path='s3a://minio-sink-bucket/delta/bronze/customers_stream/data'\n",
    "spark.sql(\"CREATE TABLE \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeea2079-f28a-4463-8507-78b8b621d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaDf = DeltaTable.forName(spark, 'customerstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9527564-9dd9-4ede-a152-3437f6681128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergetoDF(microDF, batchId):\n",
    "    print(f\"inside foreachBatch for batchId{batchId}. rows passed={microDF.count()}\")\n",
    "#    microDF.dropDuplicates(\"id\")\n",
    "    deltaDf.alias(\"t\").merge(microDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6806d0-d9b4-428f-9355-b3d6a12f9c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 06:36:18 WARN MicroBatchExecution: The read limit MaxFiles: 1000 for DeltaSource[s3a://minio-sink-bucket/delta/bronze/customers/data] is ignored when Trigger.Once() is used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fcab4488250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside foreachBatch for batchId1. rows passed=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .foreachBatch(mergetoDF) \\\n",
    "  .trigger(once=True) \\\n",
    "  .option(\"checkpointLocation\", 's3a://minio-sink-bucket/delta/bronze/customers_stream/checkpoints/') \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ab6ff-76c0-4646-b9e4-059b7bd5b380",
   "metadata": {},
   "source": [
    "the following is an example of stream data with upsert, in order to avoid duplication by id\n",
    "the input is CSV file and the output is bronze delta with only rellevant id's\n",
    "we might want to change the CSV to Avro, as its more efficient, but less readable\n",
    "now, execute the insert and update from the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658e366a-4ff6-40c6-b2b0-938916311cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+--------------------+--------------------+\n",
      "|  id|first_name|last_name|               email|            inserted|\n",
      "+----+----------+---------+--------------------+--------------------+\n",
      "|1001|      Jane|  changed|sally.thomas@acme...|2021-10-31 06:35:...|\n",
      "|1004|      Anne|Kretchmar|  annek@noanswer.org|2021-10-31 06:31:...|\n",
      "|1005|      John|      Doe|john.doe@example.com|2021-10-31 06:35:...|\n",
      "|1002|    George|   Bailey|  gbailey@foobar.com|2021-10-31 06:31:...|\n",
      "|1003|    Edward|   Walker|       ed@walker.com|2021-10-31 06:31:...|\n",
      "+----+----------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaDf.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6c49b7-bec5-45a7-b485-e60a02200be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|       operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      1|2021-10-31 06:35:46|  null|    null|STREAMING UPDATE|{outputMode -> Ap...|null|    null|     null|          0|          null|         true|{numRemovedFiles ...|        null|\n",
      "|      0|2021-10-31 06:31:30|  null|    null|STREAMING UPDATE|{outputMode -> Ap...|null|    null|     null|       null|          null|         true|{numRemovedFiles ...|        null|\n",
      "+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53160895-6f94-4111-a3c9-376aea8f3549",
   "metadata": {},
   "source": [
    "example for updating according to input event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56780423-6ddf-4624-8ed3-d13fe0a6a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa851701-0949-4d3e-94c3-7a2dfe063ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-----+--------+\n",
      "| id|first_name|last_name|email|inserted|\n",
      "+---+----------+---------+-----+--------+\n",
      "+---+----------+---------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cdd2ed-3d64-4d36-b2e2-cbc31fbb2756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 15 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d0caf-c410-4cdc-ab49-c091b4eb4329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
